<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning on Nzo&#39;s Blog</title>
    <link>http://nianze.tk/en/tags/machine-learning/</link>
    <description>Recent content in machine learning on Nzo&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>daoxinzhishui@gmail.com (Nzo)</managingEditor>
    <webMaster>daoxinzhishui@gmail.com (Nzo)</webMaster>
    <copyright>©{year}, All Rights Reserved</copyright>
    <lastBuildDate>Thu, 04 Jan 2018 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://nianze.tk/en/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    
      
      <item>
        <title>Machine learning overview</title>
        <link>http://nianze.tk/en/notes/2018/01/start-of-machine-learning-series/</link>
        <pubDate>Thu, 04 Jan 2018 00:00:00 +0000</pubDate>
        <author>daoxinzhishui@gmail.com (Nzo)</author>
        <guid>http://nianze.tk/en/notes/2018/01/start-of-machine-learning-series/</guid>
        <description>&lt;p&gt;My first post in the new &lt;code&gt;machine learning&lt;/code&gt; series.&lt;/p&gt;
&lt;!-- toc --&gt;
&lt;p&gt;Although I&#39;ve chosen &lt;code&gt;https://nianze.ml&lt;/code&gt; as my personal website domain name, I haven&#39;t really posted any article on &lt;strong&gt;M&lt;/strong&gt;achine &lt;strong&gt;L&lt;/strong&gt;earning at all, which may somehow be &lt;em&gt;misleading&lt;/em&gt;. Considering it&#39;s new year and my website has just been re-designed, it&#39;s perfect time for new plans, so I&#39;ve made a dicision to begin a new series related to &lt;code&gt;ml&lt;/code&gt;: I&#39;ll write down learning notes during my self-study in machine learning. Recently I&#39;m reading the book &lt;a href=&#34;https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/&#34;&gt;&lt;em&gt;Hands-On Machine Learning with Scikit-Learn and TensorFlow&lt;/em&gt;&lt;/a&gt; by Aurélien Géron, which should be a good start for this new series.&lt;/p&gt;
&lt;p&gt;At first the post is intended to be written in Chinese, but considering there&#39;re so many technique terms in English that I do not know the exact Chinese translation, I&#39;ll just start with English.&lt;/p&gt;
&lt;p&gt;As the first post in this series, let&#39;s just take a overview on machine learning system.&lt;/p&gt;
&lt;h1 id=&#34;types-of-machine-learning&#34;&gt;Types of machine learning&lt;/h1&gt;
&lt;p&gt;There are broadly three ways to classify machine learning systems, and each of these three could be further categorized into multiple sub-categories:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;Machine learning system
├── trained with supervision or without
│   ├── supervised learning
│   │   ├── k-Nearest Neighbors
│   │   ├── Linear Regression
│   │   ├── Logistic Regression
│   │   ├── Support Vector Machines &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;SVMs&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │   ├── Decision Trees and Random Forests
│   │   └── Neural networks
│   ├── unsupervised learning
│   │   ├── clustering
│   │   │   ├── k-Means
│   │   │   ├── Hierarchical Cluster Analysis &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;HCA&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │   │   └── Expectation Maximization
│   │   ├── Visualization and dimensionality reduction
│   │   │   ├── Principal Component Analysis &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;PCA&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │   │   ├── Kernel PCA
│   │   │   ├── Locally-Linear Embedding &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;LLE&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │   │   └── t-distributed Stochastic Neighbor Embedding &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;t-SNE&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │   └── Association rule learning
│   │       ├── Apriori
│   │       └── Eclat
│   ├── semisupervised learning
│   └── reinforcement learning
├── learn incrementally or in a whole batch
│   ├── online learning &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;incremental learning&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   │       ├── adapting rapidly to changing data and autonomous system
│   │       └── out-of-core learning &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;training on large quantities of data&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
│   └── batch learning
└── predict based on a model or not
    ├── instance-based learning &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;using a similarity measure&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
    └── model-based learning
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h1 id=&#34;main-challenges-of-machine-learning&#34;&gt;Main challenges of machine learning&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Bad data
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/35179.pdf&#34;&gt;Insufficient quantity&lt;/a&gt; of training data&lt;/li&gt;
&lt;li&gt;Nonrepresentative training data&lt;/li&gt;
&lt;li&gt;Poor-quality data&lt;/li&gt;
&lt;li&gt;Irrelevant features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Bad algorithm
&lt;ul&gt;
&lt;li&gt;Overfitting the training data&lt;/li&gt;
&lt;li&gt;Underfitting the training data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We reduce overfitting by constraining the degrees of freedom the model has, which is called &lt;em&gt;regularization&lt;/em&gt;. The amount of regularization can be controlled by a hyperparameter, which is a parameter of the learning algorithm (not of the model). The larger the hyperparameter, the smaller the model parameter, ending up with more constrain we apply to the model and less degrees of freedom.&lt;/p&gt;
&lt;p&gt;On the other side, to solve underfitting problem, we may consider:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;select more powerful model with more parameters&lt;/li&gt;
&lt;li&gt;feed better fetures&lt;/li&gt;
&lt;li&gt;reduce the constraints (e.g., reducing the regularization hyperparameter)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;testing-and-validating&#34;&gt;Testing and validating&lt;/h1&gt;
&lt;p&gt;Usually we split data into three groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;training set&lt;/li&gt;
&lt;li&gt;validation set&lt;/li&gt;
&lt;li&gt;test set&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And take following common workflow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;train multiple models with various hyperparameters using the &lt;strong&gt;training set&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;select the model and hyperparameters tht perform best on the &lt;strong&gt;validation set&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;run a single final tets against the &lt;strong&gt;test set&lt;/strong&gt; to get an estimate of the &lt;em&gt;generalization error&lt;/em&gt; (&lt;em&gt;out-of-sample error&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Further, we can use cross-validation technique to reuse data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;split trainig set into complementary subsets&lt;/li&gt;
&lt;li&gt;train each model against a different combination of these subsets and validate against the remaining parts&lt;/li&gt;
&lt;li&gt;select the model type and hyperparameters with best performance&lt;/li&gt;
&lt;li&gt;train the final model by feeding the full training set to the chosen model and hyperparameters&lt;/li&gt;
&lt;li&gt;measure the generalized error on the test set&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;concept-checkout&#34;&gt;Concept checkout:&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;How would you define Machine Learning?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ML is a system that can learn from data. Specifically, given performance measure, the learning will result in better performance at some tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can you name four types of problems where it shines?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Complex problems without known algorithmic solution&lt;/li&gt;
&lt;li&gt;Long hand-tuned rules&lt;/li&gt;
&lt;li&gt;System that needs to adapt to fluctuating environment&lt;/li&gt;
&lt;li&gt;Data mining (help humans learn)&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is a labeled training set?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&#39;s a training set that contains the desired solution (a &lt;em&gt;label&lt;/em&gt;) for each instance&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What are the two most common supervised tasks?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regression and classification&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What is the purpose of test set and validation set?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production.&lt;/li&gt;
&lt;li&gt;A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Why would you prefer cross-validation?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate validation set. This saves precious training data.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
      </item>
      
    
  </channel>
</rss>